# Kokoro Timestamp Janky

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A robust method for generating accurate, phrase-level timestamps for high-quality audio synthesized by the Kokoro TTS model. This project provides a solution to the common TTS challenge: achieving both superior audio quality and precise temporal metadata.

## The Motivation: Quality vs. Timestamps

The primary driver for this project was the goal of creating an automated pipeline to convert news articles from the website [noticiasresumidas.com](https://noticiasresumidas.com/noticias) into narrated video content. Achieving this required a high-quality, natural-sounding voice for narration, which led to the selection of the Kokoro Text-to-Speech (TTS) engine for its superior Spanish voice quality.

However, a fundamental challenge arises when attempting to generate synchronized subtitles or metadata for this video content.

The core conflict is:
1.  **Maximum Audio Quality:** The most natural-sounding audio, with proper prosody, intonation, and cadence, is achieved when the model performs a **single inference pass** on a large, continuous block of text. The model uses contextual cues like commas and periods to create realistic pauses and intonational shifts.
2.  **Accurate Timestamps:** The most straightforward way to get timestamps is to split the text into smaller chunks (phrases or sentences), generate audio for each chunk individually, and record the duration.

Attempting to merge these individually generated chunks results in audibly inferior quality. The result often sounds disjointed and robotic, with unnatural silences and a repetitive cadence, as the model loses the broader context of the paragraph.

This project was born to resolve this dilemma, providing a pipeline to generate timestamps that are precisely synchronized with the superior, single-pass audio, making it ideal for creating high-quality narrated videos.

## The Technical Challenge: A Deeper Dive

The problem can be framed as a choice between two inference methodologies:

*   **Single-Pass Inference:**
    *   **Pros:** Produces a cohesive audio stream with natural prosody. Pauses between sentences are generated by the model, reflecting human speech patterns.
    *   **Cons:** The model outputs a single, monolithic audio stream. It does not natively provide metadata indicating where one sentence ends and the next begins.

*   **Chunk-Based Inference & Concatenation:**
    *   **Pros:** By generating audio for each phrase, we can perfectly measure its duration. This allows for the trivial creation of a timestamp file.
    *   **Cons:** This approach introduces significant audio artifacts:
        *   **Loss of Context:** Each chunk is synthesized in isolation, leading to a repetitive terminal intonation (the "end-of-statement" drop in pitch).
        *   **Unnatural Pauses:** The silences inserted between chunks are digital and absolute, lacking the subtle ambient noise of a natural pause.
        *   **Concatenation Clicks:** Joining raw audio waveforms can produce audible clicks or pops at the seams.

This project implements a third way: **The Calibration Method**. It uses the predictable timing of the chunk-based method as a "scaffold" to map timestamps onto the high-quality, single-pass audio.

## Alternatives Explored

Before committing to this solution, several other open-source TTS engines were evaluated for their Spanish voice offerings and ease of integration:

*   **Piper TTS:** A highly recommended alternative. It's extremely fast, ONNX-based, and offers a wide variety of high-quality Spanish voices. It's an excellent choice for applications requiring low-latency, offline synthesis.
*   **Coqui TTS:** A comprehensive and powerful TTS toolkit with a large community and numerous pre-trained models. It provides a more framework-oriented approach to both inference and training.
*   **Bark:** A generative audio model from Suno. It's capable of producing incredibly expressive and realistic audio, including non-speech sounds. However, its inference speed is significantly slower, making it unsuitable for generating long-form audio content efficiently.

Ultimately, the unique timbre and clarity of the Kokoro "Dora" voice remained the preferred choice, necessitating the development of this custom timestamping solution.

## How It Works: The Calibration Pipeline

The "janky" in the project name refers to this clever, multi-step calibration process, which is fully automated by the main script:

1.  **Direct Generation (Gold Standard):** The script first performs a single inference pass on the entire input text to generate `unico_directo.wav`. This is our final, high-quality audio target.
2.  **Text Chunking:** The input text is intelligently split into smaller phrases based on sentence-ending punctuation (`.`) while respecting configurable character length constraints.
3.  **Iterative Inference:** The script iterates through each phrase, generating its corresponding audio in-memory (without writing to disk).
4.  **Baseline Timestamp Creation:** As each in-memory audio chunk is generated, its precise duration is measured. A "baseline" timestamp file is constructed, mapping each phrase to its start and end times within a theoretical timeline composed of all chunks plus user-defined silences.
5.  **Correction Factor Calculation:** This is the core of the solution. The script measures the exact duration of the `unico_directo.wav` and the total theoretical duration of the concatenated audio. It then computes a single floating-point number:
    `scaling_factor = duration_direct_audio / duration_merged_audio`
    This factor (typically a value like `0.925`) encapsulates the cumulative time saved by the model's more efficient, natural pacing in the single-pass inference.
6.  **Timestamp Calibration:** The script iterates through the baseline timestamps and multiplies every `start_ms` and `end_ms` value by the calculated `scaling_factor`.
7.  **Final Output:** The process concludes by:
    *   Converting the high-quality `unico_directo.wav` to MP3 using `ffmpeg`.
    *   Deleting the temporary `.wav` file.
    *   Saving a `timestamps_corregidos.json` file containing the calibrated timestamps, now perfectly synchronized with the final MP3 audio.

## Installation

This project has been tested on **Windows 10/11 using Python 3.11**.

### Prerequisites

The following software must be installed and available in your system's PATH.

1.  **Python 3.11:** [Download from python.org](https://www.python.org/downloads/). **Crucially, check the "Add Python to PATH" box during installation.**
2.  **Git:** [Download from git-scm.com](https://git-scm.com/downloads).
3.  **eSpeak NG:** A critical dependency for text-to-phoneme conversion.
    *   [Download the `.msi` installer from GitHub Releases](https://github.com/espeak-ng/espeak-ng/releases).
    *   Install it to the default location (`C:\Program Files\eSpeak NG`). The script relies on this path.
4.  **ffmpeg:** Required for the final WAV to MP3 conversion.
    *   [Download the binaries from the official website](https://ffmpeg.org/download.html).
    *   Extract the archive and add its `bin` subdirectory to your system's PATH environment variable.

### Project Setup

```bash
# 1. Clone this repository
git clone https://github.com/your-username/kokoro_timestamp_janky.git

# 2. Navigate into the project directory
cd kokoro_timestamp_janky

# 3. Create a Python virtual environment
python -m venv venv

# 4. Activate the virtual environment
# On Windows (CMD/PowerShell):
.\venv\Scripts\activate
# On macOS/Linux:
# source venv/bin/activate

# 5. Install Python dependencies
pip install -r requirements.txt

# 6. Download the Kokoro models
# Go to: https://huggingface.co/leonelhs/kokoro-thewh1teagle/tree/main
# Download the following two files:
# - kokoro-v1.0.onnx
# - voices-v1.0.bin
# Create a folder named 'model' in the project root and place both files inside.
```

## Usage

1.  Open the main script (e.g., `main.py`) in a text editor.
2.  Modify the `INPUT_TEXT` variable with the content you wish to synthesize.
3.  (Optional) Adjust other configuration parameters at the top of the file, such as `VOICE_CHOICE`, `SPEED`.
4.  Run the script from your terminal (ensure the virtual environment is active):

```bash
python main.py
```

### Project Outputs

Upon successful execution, two primary files will be generated:

*   `unico_directo.mp3`: The final, high-quality MP3 audio file.
*   `timestamps_corregidos.json`: A JSON file containing an array of objects, each mapping a text phrase to its calibrated start and end times in milliseconds, synchronized with the MP3 file.

## Project Structure

```
kokoro_timestamp_janky/
│
├── model/
│   ├── kokoro-v1.0.onnx
│   └── voices-v1.0.bin
│
├── venv/
│
├── main.py                 # The main executable script
├── requirements.txt        # Python package dependencies
└── README.md               # This file
```

## Contributing

Suggestions and improvements are **greatly appreciated**. If you have an idea to make this process less "janky" or more robust, please feel free to open an issue or submit a pull request.

Please see the `CONTRIBUTING.md` file for details on our code of conduct, and the process for submitting pull requests to us. You can also open an [Issue](https://github.com/EscribanoFNR/kokoro_timestamp_janky/issues) to report a bug or suggest a new feature.

## License

This project is distributed under the MIT License. See the `LICENSE` file for more details.

## Acknowledgements

This project would not exist without the foundational work of several incredible open-source developers and projects. Their efforts have made high-quality, local Text-to-Speech accessible to everyone, and this tool is built directly upon their contributions.

Special thanks and attribution go to the following:

*   **The Kokoro Project by `hexgrad`**: For creating the original high-quality TTS model and training the voices that are at the heart of this tool. Their pioneering work is the fundamental building block for everything here.
    *   [Kokoro GitHub Repository](https://github.com/hexgrad/kokoro)
    *   [Kokoro-82M Model Card](https://huggingface.co/hexgrad/Kokoro-82M)

*   **`thewh1teagle` for `kokoro-onnx`**: For the essential work of converting the original model to the ONNX format and creating the Python library that makes local inference straightforward and efficient. This project directly depends on this implementation to run the model.
    *   [kokoro-onnx GitHub Repository](https://github.com/thewh1teagle/kokoro-onnx)

*   **`hexgrad` for `Misaki`**: For developing the robust phonemization library (`espeak-ng` wrapper) that accurately converts Spanish text into the phoneme sequences required as input by the Kokoro model.
    *   [Misaki GitHub Repository](https://github.com/hexgrad/misaki)

*   **`leonelhs` for the `kokoro-tts-spanish` Hugging Face Space**: The simple and effective `app.py` in this Space served as the initial inspiration and a clear demonstration of Kokoro's potential, ultimately sparking the idea that led to the development of this timestamping project.
    *   [kokoro-tts-spanish Hugging Face Space](https://huggingface.co/spaces/leonelhs/kokoro-tts-spanish/blob/main/app.py)

The collaborative spirit of the open-source community is what makes tools like this possible. We are deeply grateful to all the authors and contributors of these projects.
