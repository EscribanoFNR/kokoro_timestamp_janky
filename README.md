# Kokoro Timestamp Janky

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A robust method for generating accurate, phrase-level timestamps for high-quality audio synthesized by the Kokoro TTS model. This project provides a solution to the common TTS challenge: achieving both superior audio quality and precise temporal metadata.

## The Motivation: Quality vs. Timestamps

The primary driver for this project was the goal of creating an automated pipeline to convert news articles from the website [noticiasresumidas.com](https://noticiasresumidas.com/noticias) into narrated video content. Achieving this required a high-quality, natural-sounding voice for narration, which led to the selection of the Kokoro Text-to-Speech (TTS) engine for its superior Spanish voice quality.

However, a fundamental challenge arises when attempting to generate synchronized subtitles or metadata for this video content.

The core conflict is:
1.  **Maximum Audio Quality:** The most natural-sounding audio, with proper prosody, intonation, and cadence, is achieved when the model performs a **single inference pass** on a large, continuous block of text. The model uses contextual cues like commas and periods to create realistic pauses and intonational shifts.
2.  **Accurate Timestamps:** The most straightforward way to get timestamps is to split the text into smaller chunks (phrases or sentences), generate audio for each chunk individually, and record the duration.

Attempting to merge these individually generated chunks results in audibly inferior quality. The result often sounds disjointed and robotic, with unnatural silences and a repetitive cadence, as the model loses the broader context of the paragraph.

This project was born to resolve this dilemma, providing a pipeline to generate timestamps that are precisely synchronized with the superior, single-pass audio, making it ideal for creating high-quality narrated videos.

## The Technical Challenge: A Deeper Dive

The problem can be framed as a choice between two inference methodologies:

*   **Single-Pass Inference:**
    *   **Pros:** Produces a cohesive audio stream with natural prosody. Pauses between sentences are generated by the model, reflecting human speech patterns.
    *   **Cons:** The model outputs a single, monolithic audio stream. It does not natively provide metadata indicating where one sentence ends and the next begins.

*   **Chunk-Based Inference & Concatenation:**
    *   **Pros:** By generating audio for each phrase, we can perfectly measure its duration. This allows for the trivial creation of a timestamp file.
    *   **Cons:** This approach introduces significant audio artifacts:
        *   **Loss of Context:** Each chunk is synthesized in isolation, leading to a repetitive terminal intonation (the "end-of-statement" drop in pitch).
        *   **Unnatural Pauses:** The silences inserted between chunks are digital and absolute, lacking the subtle ambient noise of a natural pause.
        *   **Concatenation Clicks:** Joining raw audio waveforms can produce audible clicks or pops at the seams.

This project implements a third way: **The Calibration Method**. It uses the predictable timing of the chunk-based method as a "scaffold" to map timestamps onto the high-quality, single-pass audio.

## Alternatives Explored

Before committing to this solution, several other open-source TTS engines were evaluated for their Spanish voice offerings and ease of integration:

*   **Piper TTS:** A highly recommended alternative. It's extremely fast, ONNX-based, and offers a wide variety of high-quality Spanish voices. It's an excellent choice for applications requiring low-latency, offline synthesis.
*   **Coqui TTS:** A comprehensive and powerful TTS toolkit with a large community and numerous pre-trained models. It provides a more framework-oriented approach to both inference and training.
*   **Bark:** A generative audio model from Suno. It's capable of producing incredibly expressive and realistic audio, including non-speech sounds. However, its inference speed is significantly slower, making it unsuitable for generating long-form audio content efficiently.

Ultimately, the unique timbre and clarity of the Kokoro "Dora" voice remained the preferred choice, necessitating the development of this custom timestamping solution.

## How It Works: The Calibration Pipeline

The "janky" in the project name refers to this clever, multi-step calibration process, which is fully automated by the main script:

1.  **Direct Generation (Gold Standard):** The script first performs a single inference pass on the entire input text to generate `unico_directo.wav`. This is our final, high-quality audio target.
2.  **Text Chunking:** The input text is intelligently split into smaller phrases based on sentence-ending punctuation (`.`) while respecting configurable character length constraints.
3.  **Iterative Inference:** The script iterates through each phrase, generating its corresponding audio in-memory (without writing to disk).
4.  **Baseline Timestamp Creation:** As each in-memory audio chunk is generated, its precise duration is measured. A "baseline" timestamp file is constructed, mapping each phrase to its start and end times within a theoretical timeline composed of all chunks plus user-defined silences.
5.  **Correction Factor Calculation:** This is the core of the solution. The script measures the exact duration of the `unico_directo.wav` and the total theoretical duration of the concatenated audio. It then computes a single floating-point number:
    `scaling_factor = duration_direct_audio / duration_merged_audio`
    This factor (typically a value like `0.925`) encapsulates the cumulative time saved by the model's more efficient, natural pacing in the single-pass inference.
6.  **Timestamp Calibration:** The script iterates through the baseline timestamps and multiplies every `start_ms` and `end_ms` value by the calculated `scaling_factor`.
7.  **Final Output:** The process concludes by:
    *   Converting the high-quality `unico_directo.wav` to MP3 using `ffmpeg`.
    *   Deleting the temporary `.wav` file.
    *   Saving a `timestamps_corregidos.json` file containing the calibrated timestamps, now perfectly synchronized with the final MP3 audio.

## Installation

This project has been tested on **Windows 10/11 using Python 3.11**.

### Prerequisites

The following software must be installed and available in your system's PATH.

1.  **Python 3.11:** [Download from python.org](https://www.python.org/downloads/). **Crucially, check the "Add Python to PATH" box during installation.**
2.  **Git:** [Download from git-scm.com](https://git-scm.com/downloads).
3.  **eSpeak NG:** A critical dependency for text-to-phoneme conversion.
    *   [Download the `.msi` installer from GitHub Releases](https://github.com/espeak-ng/espeak-ng/releases).
    *   Install it to the default location (`C:\Program Files\eSpeak NG`). The script relies on this path.
4.  **ffmpeg:** Required for the final WAV to MP3 conversion.
    *   [Download the binaries from the official website](https://ffmpeg.org/download.html).
    *   Extract the archive and add its `bin` subdirectory to your system's PATH environment variable.

### Project Setup

```bash
# 1. Clone this repository
git clone https://github.com/your-username/kokoro_timestamp_janky.git

# 2. Navigate into the project directory
cd kokoro_timestamp_janky

# 3. Create a Python virtual environment
python -m venv venv

# 4. Activate the virtual environment
# On Windows (CMD/PowerShell):
.\venv\Scripts\activate
# On macOS/Linux:
# source venv/bin/activate

# 5. Install Python dependencies
pip install -r requirements.txt

# 6. Download the Kokoro models
# Go to: https://huggingface.co/leonelhs/kokoro-thewh1teagle/tree/main
# Download the following two files:
# - kokoro-v1.0.onnx
# - voices-v1.0.bin
# Create a folder named 'model' in the project root and place both files inside.
```

## Usage

1.  Open the main script (e.g., `main.py`) in a text editor.
2.  Modify the `INPUT_TEXT` variable with the content you wish to synthesize.
3.  (Optional) Adjust other configuration parameters at the top of the file, such as `VOICE_CHOICE`, `SPEED`.
4.  Run the script from your terminal (ensure the virtual environment is active):

```bash
python main.py
```

### Project Outputs

Upon successful execution, two primary files will be generated:

*   `unico_directo.mp3`: The final, high-quality MP3 audio file.
*   `timestamps_corregidos.json`: A JSON file containing an array of objects, each mapping a text phrase to its calibrated start and end times in milliseconds, synchronized with the MP3 file.

## Project Structure

```
kokoro_timestamp_janky/
│
├── model/
│   ├── kokoro-v1.0.onnx
│   └── voices-v1.0.bin
│
├── venv/
│
├── main.py                 # The main executable script
├── requirements.txt        # Python package dependencies
└── README.md               # This file
```

## Hardware Acceleration with NVIDIA (CUDA)

For users with a compatible NVIDIA GPU, the audio synthesis process can be significantly accelerated by offloading the computation from the CPU to the GPU. This is highly recommended when generating large volumes of audio, as it dramatically reduces processing time.

This project leverages Microsoft's ONNX Runtime, which can use NVIDIA's CUDA platform to perform model inference. The following steps outline how to configure your environment to enable this feature.

### Prerequisites

*   An NVIDIA GPU with support for CUDA. Most modern NVIDIA GPUs (GTX 10-series / RTX 20-series and newer) are compatible.
*   The latest NVIDIA Game Ready or Studio drivers for your GPU.

### Configuration Steps

#### 1. Install the GPU-Enabled Python Package

The core Python dependency needs to be installed with the `[gpu]` extra, which ensures the correct version of ONNX Runtime is downloaded.

If you have already installed the standard `requirements.txt`, first uninstall the CPU version and then install the GPU version:

```bash
# Ensure your virtual environment is active
pip uninstall onnxruntime
pip install kokoro-onnx[gpu]
```

This command automatically installs `onnxruntime-gpu` instead of the standard `onnxruntime`.

#### 2. Install NVIDIA CUDA Toolkit and cuDNN

`onnxruntime-gpu` is extremely specific about which versions of the CUDA Toolkit and cuDNN library it works with. You must install the versions that match your installed version of `onnxruntime-gpu`.

1.  **Check your `onnxruntime-gpu` version:**
    ```bash
    pip show onnxruntime-gpu
    ```
    Note the version number (e.g., `1.22.0`).

2.  **Find the Required CUDA/cuDNN Versions:**
    Go to the official ONNX Runtime documentation to find the compatibility table. This is your single source of truth.
    *   **URL:** [ONNX Runtime Execution Providers: CUDA - Requirements](https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements)

3.  **Install the CUDA Toolkit:**
    *   Based on the table, download the correct version of the CUDA Toolkit. If you need a specific older version, you can find it in the official archive.
    *   **URL:** [NVIDIA CUDA Toolkit Archive](https://developer.nvidia.com/cuda-toolkit-archive)

4.  **Install the cuDNN Library:**
    *   cuDNN is a library that you must install manually.
    *   **URL:** [NVIDIA cuDNN Archive](https://developer.nvidia.com/rdp/cudnn-archive) (Requires a free NVIDIA Developer account).
    *   On the archive page, find and download the **ZIP file** for the cuDNN version that matches your CUDA Toolkit version (e.g., `Download cuDNN v9.x.x for CUDA 12.x`).
    *   Unzip the downloaded file.
    *   Copy the contents of the `bin`, `include`, and `lib` folders from the unzipped archive into the corresponding folders in your CUDA Toolkit installation directory (e.g., `C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.x`).

### Verification

To confirm that the GPU is being used:

1.  **Run the script:** Execute `python main.py` as usual. If there are any version mismatches, you will see errors in the console related to missing `.dll` files (like `cudnn64_9.dll`). If there are no errors, proceed to the next step.
2.  **Monitor GPU Activity:**
    *   **On Windows:** Open the Task Manager, go to the **Performance** tab, and click on your NVIDIA GPU. While the script is running, you should see a spike in activity on the **CUDA** graph.
    *   **On Linux:** Run the command `watch -n 1 nvidia-smi` in a separate terminal. You should see Python appear as a process and GPU utilization increase while the script is generating audio.

If you see GPU activity, your setup is correct and you are now benefiting from hardware acceleration.

## Troubleshooting & FAQ

**Q: I get an error about `cudnn64_X.dll` being missing or a `UserWarning` that CUDA is not available.**

**A:** This is the most common issue and is almost always a version mismatch. Double-check these points:
1.  Did you install `kokoro-onnx[gpu]`?
2.  Go to the [ONNX Runtime requirements page](https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements) and confirm that your installed versions of **`onnxruntime-gpu`**, **CUDA Toolkit**, and **cuDNN** are all compatible with each other.
3.  Ensure you copied the cuDNN files (`.dll`, `.h`, `.lib`) into the correct CUDA Toolkit folders.

**Q: Can I use an AMD or Intel GPU for acceleration?**

**A:** Unfortunately, no. CUDA is a proprietary technology exclusive to NVIDIA GPUs. The `onnxruntime-gpu` package is compiled specifically against CUDA libraries and will not work with GPUs from other manufacturers.

**Q: I see warnings like `Memcpy nodes are added` or `Some nodes were not assigned`. Is this an error?**

**A:** No, these are normal informational warnings from ONNX Runtime. They indicate that the model is being processed correctly on the GPU. The `Memcpy` warning means data is being moved between your main RAM and the GPU's VRAM, and the `nodes not assigned` warning means ONNX is intelligently running a few minor operations on the CPU for efficiency. These do not affect the final audio output.

## Contributing

Suggestions and improvements are **greatly appreciated**. If you have an idea to make this process less "janky" or more robust, please feel free to open an issue or submit a pull request.

Please see the `CONTRIBUTING.md` file for details on our code of conduct, and the process for submitting pull requests to us. You can also open an [Issue](https://github.com/EscribanoFNR/kokoro_timestamp_janky/issues) to report a bug or suggest a new feature.

## License

This project is distributed under the MIT License. See the `LICENSE` file for more details.

## Acknowledgements

This project would not exist without the foundational work of several incredible open-source developers and projects. Their efforts have made high-quality, local Text-to-Speech accessible to everyone, and this tool is built directly upon their contributions.

Special thanks and attribution go to the following:

*   **The Kokoro Project by `hexgrad`**: For creating the original high-quality TTS model and training the voices that are at the heart of this tool. Their pioneering work is the fundamental building block for everything here.
    *   [Kokoro GitHub Repository](https://github.com/hexgrad/kokoro)
    *   [Kokoro-82M Model Card](https://huggingface.co/hexgrad/Kokoro-82M)

*   **`thewh1teagle` for `kokoro-onnx`**: For the essential work of converting the original model to the ONNX format and creating the Python library that makes local inference straightforward and efficient. This project directly depends on this implementation to run the model.
    *   [kokoro-onnx GitHub Repository](https://github.com/thewh1teagle/kokoro-onnx)

*   **`hexgrad` for `Misaki`**: For developing the robust phonemization library (`espeak-ng` wrapper) that accurately converts Spanish text into the phoneme sequences required as input by the Kokoro model.
    *   [Misaki GitHub Repository](https://github.com/hexgrad/misaki)

*   **`leonelhs` for the `kokoro-tts-spanish` Hugging Face Space**: The simple and effective `app.py` in this Space served as the initial inspiration and a clear demonstration of Kokoro's potential, ultimately sparking the idea that led to the development of this timestamping project.
    *   [kokoro-tts-spanish Hugging Face Space](https://huggingface.co/spaces/leonelhs/kokoro-tts-spanish/blob/main/app.py)

The collaborative spirit of the open-source community is what makes tools like this possible. We are deeply grateful to all the authors and contributors of these projects.
